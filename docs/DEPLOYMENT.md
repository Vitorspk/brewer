# Deployment Guide - Brewer Application on Amazon EKS

Este guia fornece instru√ß√µes completas para deploy da aplica√ß√£o Brewer no Amazon EKS (Elastic Kubernetes Service).

## üìã Pr√©-requisitos

### Ferramentas Necess√°rias
- AWS CLI (v2+): `aws --version`
- kubectl: `kubectl version --client`
- Docker: `docker --version`
- eksctl (recomendado): `eksctl version`
- gh CLI (para PRs): `gh --version`

### Permiss√µes AWS
- Acesso ao ECR (Elastic Container Registry)
- Acesso ao EKS
- Acesso ao S3 (para fotos)
- Acesso ao RDS MySQL (se usar RDS)
- **Nota**: LoadBalancer n√£o √© mais necess√°rio - usamos ClusterIP + Ingress

## üèóÔ∏è Infraestrutura AWS

### 1. Criar ECR Repository

```bash
aws ecr create-repository \
    --repository-name brewer \
    --region sa-east-1 \
    --image-scanning-configuration scanOnPush=true
```

### 2. Criar EKS Cluster (se ainda n√£o existe)

```bash
eksctl create cluster \
    --name brewer-cluster \
    --region sa-east-1 \
    --nodegroup-name brewer-nodes \
    --node-type t3.medium \
    --nodes 2 \
    --nodes-min 2 \
    --nodes-max 4 \
    --managed
```

### 3. Configurar kubectl

```bash
aws eks update-kubeconfig \
    --name brewer-cluster \
    --region sa-east-1
```

### 4. Instalar Nginx Ingress Controller

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.11.3/deploy/static/provider/cloud/deploy.yaml
```

Aguarde o LoadBalancer ser provisionado:

```bash
kubectl wait --namespace ingress-nginx \
  --for=condition=ready pod \
  --selector=app.kubernetes.io/component=controller \
  --timeout=120s
```

### 5. Instalar Metrics Server (para HPA)

O Metrics Server √© instalado automaticamente pelo workflow CI/CD, mas voc√™ pode instal√°-lo manualmente:

```bash
# Usando o manifest customizado (recomendado)
kubectl apply -f k8s/cluster-infra/metrics-server.yaml

# OU usando o manifest oficial do Kubernetes
kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
```

Verificar instala√ß√£o:

```bash
# Verificar se o metrics-server est√° rodando
kubectl get deployment metrics-server -n kube-system

# Aguardar ficar dispon√≠vel
kubectl wait --for=condition=available --timeout=2m deployment/metrics-server -n kube-system

# Testar coleta de m√©tricas
kubectl top nodes
kubectl top pods -n brewer
```

## üê≥ Build e Push da Imagem

### Op√ß√£o 1: Usando o script fornecido

```bash
# Configurar vari√°veis de ambiente (opcional)
export AWS_REGION=sa-east-1
export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
export ECR_REPOSITORY=brewer
export IMAGE_TAG=v1.0.0

# Executar script
./scripts/build-and-push.sh
```

### Op√ß√£o 2: Manual

```bash
# Login no ECR
aws ecr get-login-password --region sa-east-1 | \
    docker login --username AWS --password-stdin \
    $(aws sts get-caller-identity --query Account --output text).dkr.ecr.sa-east-1.amazonaws.com

# Build
docker build -t brewer:latest .

# Tag
docker tag brewer:latest \
    $(aws sts get-caller-identity --query Account --output text).dkr.ecr.sa-east-1.amazonaws.com/brewer:latest

# Push
docker push $(aws sts get-caller-identity --query Account --output text).dkr.ecr.sa-east-1.amazonaws.com/brewer:latest
```

## üîë Configurar Secrets

Crie os secrets antes do deploy:

```bash
kubectl create secret generic brewer-secrets \
  --from-literal=DATABASE_PASSWORD='your-secure-password' \
  --from-literal=AWS_ACCESS_KEY_ID='your-access-key' \
  --from-literal=AWS_SECRET_ACCESS_KEY='your-secret-key' \
  --from-literal=MAIL_USERNAME='your-email@gmail.com' \
  --from-literal=MAIL_PASSWORD='your-app-password' \
  --from-literal=MAIL_FROM='noreply@brewer.com' \
  --namespace=brewer \
  --dry-run=client -o yaml | kubectl apply -f -
```

## üöÄ Deploy da Aplica√ß√£o

### Op√ß√£o 1: Usando o script fornecido

```bash
./scripts/deploy-to-eks.sh
```

### Op√ß√£o 2: Manual

```bash
# 1. Instalar Cluster Infrastructure (metrics-server)
kubectl apply -f k8s/cluster-infra/metrics-server.yaml
kubectl wait --for=condition=available --timeout=2m deployment/metrics-server -n kube-system

# 2. Aplicar manifests da aplica√ß√£o na ordem
kubectl apply -f k8s/base/namespace.yaml
kubectl apply -f k8s/base/resourcequota.yaml
kubectl apply -f k8s/base/configmap.yaml

# 3. Aplicar deployment com image tag correto usando sed
export ECR_REGISTRY="YOUR_ACCOUNT_ID.dkr.ecr.sa-east-1.amazonaws.com"
export ECR_REPOSITORY="brewer"
export IMAGE_TAG="latest"  # ou git commit SHA

cat k8s/base/deployment.yaml | \
  sed "s|image: brewer:latest|image: $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG|g" | \
  kubectl apply -f -

# 4. Aplicar service, HPA e PodDisruptionBudget
kubectl apply -f k8s/base/service.yaml
kubectl apply -f k8s/base/hpa.yaml
kubectl apply -f k8s/base/pdb.yaml

# 5. Aplicar Ingress (recomendado para produ√ß√£o)
kubectl apply -f k8s/base/ingress-nginx.yaml
```

### Estrat√©gia de Image Tag

**IMPORTANTE**: A aplica√ß√£o usa uma estrat√©gia de image tag para evitar drift entre o Git e o cluster:

- O arquivo `k8s/base/deployment.yaml` cont√©m `image: brewer:latest` como placeholder
- O CI/CD usa `sed` para substituir o placeholder pelo tag real antes de aplicar
- Isso garante que o manifest aplicado no cluster tenha o tag correto (commit SHA)
- **Nunca** use `kubectl set image` depois de aplicar o deployment - isso causa drift

**Por que n√£o usar `kubectl set image`?**
- Se voc√™ aplicar o manifest depois, ele reverte para `:latest`
- Causa inconsist√™ncia entre Git e cluster
- A abordagem com `sed` mant√©m tudo sincronizado

```bash
# ‚ùå N√ÉO FA√áA ISSO (causa drift):
kubectl apply -f k8s/base/deployment.yaml
kubectl set image deployment/brewer-app brewer=YOUR_ECR:sha-abc123 -n brewer

# ‚úÖ FA√áA ISSO (previne drift):
cat k8s/base/deployment.yaml | \
  sed "s|image: brewer:latest|image: $ECR_REGISTRY/$ECR_REPOSITORY:$IMAGE_TAG|g" | \
  kubectl apply -f -
```

## üìä Verificar Deploy

### Status dos Pods

```bash
# Watch pods starting
kubectl get pods -n brewer -w

# View logs
kubectl logs -f deployment/brewer-app -n brewer

# Check all resources
kubectl get all -n brewer
```

### Status do Deployment

```bash
kubectl rollout status deployment/brewer-app -n brewer
```

### Obter URL do Ingress

**Nota**: Agora usamos ClusterIP + Ingress (n√£o mais LoadBalancer direto no Service):

```bash
# Obter External IP do Nginx Ingress Controller
INGRESS_IP=$(kubectl get svc ingress-nginx-controller -n ingress-nginx -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
echo "Ingress URL: http://${INGRESS_IP}"
echo "Application Domain: http://brewer.virosistemas.com"

# Verificar status do Ingress
kubectl get ingress -n brewer
kubectl describe ingress brewer-ingress-nginx -n brewer
```

### Verificar HPA (Horizontal Pod Autoscaler)

```bash
# Status atual do HPA
kubectl get hpa -n brewer

# Watch HPA em tempo real
kubectl get hpa -n brewer -w

# Detalhes completos
kubectl describe hpa brewer-app-hpa -n brewer

# Verificar se m√©tricas est√£o dispon√≠veis
kubectl top pods -n brewer
kubectl top nodes
```

**Comportamento do HPA:**
- **minReplicas**: 2 (sempre mant√©m no m√≠nimo 2 pods)
- **maxReplicas**: 10 (escala at√© 10 pods se necess√°rio)
- **CPU target**: 70% utilization
- **Memory target**: 80% utilization
- **Scale Up**: R√°pido (60s stabilization, +100% ou +2 pods/min)
- **Scale Down**: Conservador (300s stabilization, -50% ou -1 pod/min)

### Verificar ResourceQuota

```bash
# Ver limites do namespace
kubectl describe resourcequota brewer-quota -n brewer

# Ver uso atual vs limites
kubectl get resourcequota -n brewer
```

**Limites configurados:**
- CPU requests: 4 cores (permite at√© 16 pods @ 250m cada)
- CPU limits: 8 cores (permite at√© 8 pods @ 1000m cada)
- Memory requests: 8Gi (permite at√© 16 pods @ 512Mi cada)
- Memory limits: 16Gi (permite at√© 16 pods @ 1Gi cada)
- Max pods: 20
- Max PVCs: 5
- Storage: 100Gi

**Nota**: HPA max=10 est√° bem dentro dos limites de ResourceQuota.

### Verificar PodDisruptionBudget

```bash
# Verificar PDB
kubectl get pdb -n brewer
kubectl describe pdb brewer-app-pdb -n brewer
```

O PDB garante que **minAvailable: 1** pod esteja sempre rodando durante:
- Node drains (manuten√ß√£o de n√≥s)
- Cluster upgrades
- Voluntary disruptions

### Verificar Actuator Endpoints

```bash
# Health check p√∫blico (porta 8080)
kubectl port-forward -n brewer deployment/brewer-app 8080:8080
curl http://localhost:8080/actuator/health
curl http://localhost:8080/actuator/info

# Metrics interno (porta 9090 - n√£o exposto publicamente)
kubectl port-forward -n brewer deployment/brewer-app 9090:9090
curl http://localhost:9090/actuator/metrics
curl http://localhost:9090/actuator/prometheus
```

**Nota de Seguran√ßa**: Apenas `health` e `info` est√£o expostos na porta principal. M√©tricas sens√≠veis (prometheus, metrics) est√£o isoladas na porta 9090.

## üîß Configura√ß√µes Importantes

### ConfigMap (k8s/base/configmap.yaml)

Ajuste conforme seu ambiente:
- `DATABASE_URL`: URL do MySQL (RDS ou outro)
- `AWS_REGION`: Regi√£o AWS
- `AWS_S3_BUCKET`: Nome do bucket S3
- `MAIL_HOST`: Servidor SMTP

### Deployment (k8s/base/deployment.yaml)

Ajuste recursos conforme necessidade:
```yaml
resources:
  requests:
    memory: "512Mi"
    cpu: "250m"
  limits:
    memory: "1Gi"
    cpu: "1000m"
```

### HPA (k8s/base/hpa.yaml)

Ajuste escalabilidade:
```yaml
minReplicas: 2
maxReplicas: 10
```

## üóÑÔ∏è Database (MySQL)

### Op√ß√£o 1: Amazon RDS MySQL

Recomendado para produ√ß√£o. Configure Multi-AZ para alta disponibilidade.

```bash
# Exemplo de cria√ß√£o RDS
aws rds create-db-instance \
    --db-instance-identifier brewer-db \
    --db-instance-class db.t3.micro \
    --engine mysql \
    --engine-version 8.0.35 \
    --master-username admin \
    --master-user-password YourSecurePassword \
    --allocated-storage 20 \
    --multi-az \
    --db-name brewer \
    --vpc-security-group-ids sg-xxxxx \
    --region sa-east-1
```

Atualize o ConfigMap com o endpoint do RDS.

### Op√ß√£o 2: MySQL no Kubernetes

Para desenvolvimento/testes apenas. N√£o recomendado para produ√ß√£o.

## üîÑ Atualiza√ß√µes e Rollback

### Deploy de Nova Vers√£o

```bash
# Build nova vers√£o
export IMAGE_TAG=v1.1.0
./scripts/build-and-push.sh

# Atualizar deployment
kubectl set image deployment/brewer-app \
    brewer=YOUR_ACCOUNT_ID.dkr.ecr.sa-east-1.amazonaws.com/brewer:v1.1.0 \
    -n brewer

# Acompanhar rollout
kubectl rollout status deployment/brewer-app -n brewer
```

### Rollback

```bash
# Ver hist√≥rico
kubectl rollout history deployment/brewer-app -n brewer

# Rollback para vers√£o anterior
kubectl rollout undo deployment/brewer-app -n brewer

# Rollback para revis√£o espec√≠fica
kubectl rollout undo deployment/brewer-app -n brewer --to-revision=2
```

## üìà Monitoramento

### Logs

```bash
# Logs de todos os pods
kubectl logs -f -l app=brewer -n brewer

# Logs de um pod espec√≠fico
kubectl logs -f pod/brewer-app-xxxxx -n brewer

# Logs anteriores (ap√≥s restart)
kubectl logs --previous pod/brewer-app-xxxxx -n brewer
```

### M√©tricas

```bash
# CPU e Mem√≥ria dos pods
kubectl top pods -n brewer

# Status do HPA
kubectl get hpa -n brewer -w
```

### Exec no Container

```bash
kubectl exec -it deployment/brewer-app -n brewer -- /bin/sh
```

## üõ°Ô∏è Seguran√ßa

### Melhorias de Seguran√ßa Implementadas

A aplica√ß√£o implementa v√°rias camadas de seguran√ßa:

#### 1. Actuator Endpoints Protegidos

```yaml
# Somente health e info expostos publicamente (porta 8080)
MANAGEMENT_ENDPOINTS_WEB_EXPOSURE_INCLUDE: "health,info"

# M√©tricas sens√≠veis isoladas em porta separada (porta 9090)
MANAGEMENT_SERVER_PORT: "9090"
```

**Por que isso √© importante:**
- M√©tricas Prometheus e outros endpoints sens√≠veis n√£o est√£o expostos publicamente
- Reduz superf√≠cie de ataque
- M√©tricas ainda acess√≠veis internamente para monitoring

#### 2. Rate Limiting no Ingress

```yaml
# Prote√ß√£o contra DDoS e abuso
nginx.ingress.kubernetes.io/limit-rps: "10"
nginx.ingress.kubernetes.io/limit-rpm: "100"
nginx.ingress.kubernetes.io/limit-connections: "10"
```

**Prote√ß√µes:**
- M√°ximo 10 requests por segundo por IP
- M√°ximo 100 requests por minuto por IP
- M√°ximo 10 conex√µes simult√¢neas por IP

#### 3. Session Cookie Seguro

```yaml
# Cookie de sess√£o com dura√ß√£o limitada
nginx.ingress.kubernetes.io/session-cookie-max-age: "28800"  # 8 horas
```

**Antes:** 48 horas (muito tempo para dados sens√≠veis)
**Agora:** 8 horas (balanceando UX e seguran√ßa)

#### 4. ResourceQuota (Prote√ß√£o contra Resource Exhaustion)

```yaml
# Limita recursos do namespace
requests.cpu: "4"
limits.cpu: "8"
requests.memory: 8Gi
limits.memory: 16Gi
pods: "20"
```

**Previne:**
- Pods descontrolados consumindo todo o cluster
- Ataques de resource exhaustion
- Bills inesperadas de cloud

#### 5. PodDisruptionBudget (Alta Disponibilidade)

```yaml
# Garante disponibilidade durante maintenance
minAvailable: 1
```

**Garante:**
- Pelo menos 1 pod sempre rodando
- Prote√ß√£o durante node drains e cluster upgrades
- Zero downtime durante manuten√ß√µes programadas

#### 6. Flyway Repair Otimizado

```yaml
# Job n√£o inicia aplica√ß√£o completa, apenas reparo
args:
  - "--spring.main.web-application-type=none"
  - "--spring.flyway.repair-on-migrate=true"
```

**Antes:** Iniciava toda a aplica√ß√£o Spring Boot
**Agora:** Executa apenas reparo de migrations
**Benef√≠cio:** Mais r√°pido, menos recursos, mais seguro

#### 7. Startup Probe Otimizado

```yaml
# Detecta pods lentos rapidamente
initialDelaySeconds: 0
periodSeconds: 5
failureThreshold: 60  # 5 * 60 = 300s timeout
```

**Benef√≠cio:** Detecta falhas de startup mais r√°pido, previne pods stuck

#### 8. Service Type ClusterIP

**Antes:** LoadBalancer (AWS NLB adicional = custo extra)
**Agora:** ClusterIP com Ingress (√∫nico LoadBalancer)
**Benef√≠cio:** Menos custos, mesma funcionalidade

### Secrets Management

**Importante:** Nunca commite secrets no Git!

Para ambientes de produ√ß√£o, considere usar:
- **AWS Secrets Manager + External Secrets Operator** (recomendado para AWS)
- **HashiCorp Vault**
- **Sealed Secrets**

### Network Policies

Implementar Network Policies para isolar o namespace:

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: brewer-netpol
  namespace: brewer
spec:
  podSelector:
    matchLabels:
      app: brewer
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 8080
  egress:
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 3306  # MySQL
    - protocol: TCP
      port: 443   # HTTPS
```

### Checklist de Seguran√ßa

- [x] Actuator endpoints sens√≠veis isolados (porta 9090)
- [x] Rate limiting configurado no Ingress
- [x] Session cookie com dura√ß√£o apropriada (8h)
- [x] ResourceQuota configurado
- [x] PodDisruptionBudget configurado
- [x] Startup probe otimizado
- [x] Service tipo ClusterIP (n√£o LoadBalancer)
- [x] Flyway repair otimizado
- [x] Secrets gerenciados via Kubernetes Secrets
- [ ] TLS/HTTPS configurado (pr√≥ximo passo)
- [ ] Network Policies implementadas (opcional)
- [ ] AWS Secrets Manager integrado (recomendado para produ√ß√£o)

## üß™ Testar Auto-Scaling (HPA)

### Teste de Carga Simples

```bash
# 1. Verificar estado inicial
kubectl get hpa -n brewer
kubectl get pods -n brewer

# 2. Gerar carga (usando Apache Bench)
kubectl run -i --tty load-generator --rm --image=busybox:1.28 --restart=Never -- /bin/sh -c "while sleep 0.01; do wget -q -O- http://brewer-app.brewer.svc.cluster.local/actuator/health; done"

# 3. Em outro terminal, watch HPA e pods
kubectl get hpa -n brewer -w
kubectl get pods -n brewer -w

# 4. Ver m√©tricas em tempo real
kubectl top pods -n brewer
```

### Comportamento Esperado

**Scale Up** (quando CPU > 70% ou Memory > 80%):
- HPA detecta alta utiliza√ß√£o
- Aguarda 60s de estabiliza√ß√£o
- Aumenta pods em at√© 100% ou +2 pods/min
- Pods adicionais s√£o criados
- Carga √© distribu√≠da

**Scale Down** (quando uso normaliza):
- HPA detecta baixa utiliza√ß√£o
- Aguarda 300s (5 minutos) de estabiliza√ß√£o
- Reduz pods em at√© 50% ou -1 pod/min
- Nunca desce abaixo de minReplicas: 2
- PDB garante minAvailable: 1 durante scale down

### Exemplo de Teste com Curl

```bash
# Gerar requests em loop
for i in {1..10000}; do
  curl -s http://brewer.virosistemas.com/actuator/health > /dev/null
  echo "Request $i"
  sleep 0.1
done
```

## üö® Troubleshooting

### Pod n√£o inicia

```bash
# Descrever pod para ver eventos
kubectl describe pod brewer-app-xxxxx -n brewer

# Ver logs do pod
kubectl logs brewer-app-xxxxx -n brewer

# Ver logs anteriores (se pod reiniciou)
kubectl logs --previous brewer-app-xxxxx -n brewer

# Verificar eventos do namespace
kubectl get events -n brewer --sort-by='.lastTimestamp'
```

**Problemas comuns:**
- `ImagePullBackOff`: Verificar se a imagem existe no ECR e se IRSA/credentials est√£o corretos
- `CrashLoopBackOff`: Verificar logs do pod e configura√ß√£o do ConfigMap/Secrets
- `Pending`: Verificar ResourceQuota, pode estar sem recursos dispon√≠veis

### HPA n√£o est√° escalando

```bash
# 1. Verificar se metrics-server est√° rodando
kubectl get deployment metrics-server -n kube-system
kubectl logs -n kube-system deployment/metrics-server

# 2. Verificar se m√©tricas est√£o dispon√≠veis
kubectl top nodes
kubectl top pods -n brewer

# 3. Verificar HPA status
kubectl describe hpa brewer-app-hpa -n brewer

# 4. Verificar eventos do HPA
kubectl get events -n brewer | grep HorizontalPodAutoscaler
```

**Problemas comuns:**
- M√©tricas n√£o dispon√≠veis: metrics-server n√£o est√° instalado ou com problemas
- `unable to get metrics`: Aguardar alguns minutos ap√≥s deploy
- HPA mostra `<unknown>`: Pods ainda n√£o t√™m m√©tricas suficientes
- N√£o escala: Verificar se CPU/Memory est√£o realmente acima dos targets

### Metrics Server n√£o funciona

```bash
# Verificar logs
kubectl logs -n kube-system deployment/metrics-server --tail=50

# Verificar API
kubectl get apiservices | grep metrics

# Se API n√£o est√° dispon√≠vel, reinstalar
kubectl delete -f k8s/cluster-infra/metrics-server.yaml
kubectl apply -f k8s/cluster-infra/metrics-server.yaml
kubectl wait --for=condition=available --timeout=2m deployment/metrics-server -n kube-system
```

### ResourceQuota est√° bloqueando pods

```bash
# Ver uso atual vs limites
kubectl describe resourcequota brewer-quota -n brewer

# Ver recursos de todos os pods
kubectl describe pods -n brewer | grep -A 5 "Requests:"
```

**Solu√ß√£o**: Se atingiu os limites, voc√™ tem duas op√ß√µes:
1. Reduzir recursos dos pods em `deployment.yaml`
2. Aumentar limites do ResourceQuota em `resourcequota.yaml`

### Problemas de conectividade com Ingress

```bash
# Verificar Ingress Controller
kubectl get pods -n ingress-nginx
kubectl logs -n ingress-nginx deployment/ingress-nginx-controller

# Verificar Ingress resource
kubectl describe ingress brewer-ingress-nginx -n brewer

# Verificar Service
kubectl get svc brewer-app -n brewer
kubectl get endpoints brewer-app -n brewer

# Testar conectividade interna
kubectl run test --rm -it --image=busybox --restart=Never -- wget -O- http://brewer-app.brewer.svc.cluster.local/actuator/health
```

**Rate Limiting**: Ingress tem rate limiting configurado:
- 10 RPS (requests per second)
- 100 RPM (requests per minute)
- 10 conex√µes simult√¢neas

Se estiver sendo bloqueado, ajuste em `k8s/base/ingress-nginx.yaml`.

### Problemas com secrets

```bash
# Listar secrets
kubectl get secrets -n brewer

# Descrever secret (n√£o mostra valores)
kubectl describe secret brewer-secrets -n brewer

# Ver secret (base64 encoded)
kubectl get secret brewer-secrets -n brewer -o yaml

# Decodificar um valor espec√≠fico
kubectl get secret brewer-secrets -n brewer -o jsonpath='{.data.DATABASE_PASSWORD}' | base64 -d
```

### Flyway Migration falha

```bash
# Ver logs do job de reparo
kubectl logs -n brewer job/flyway-repair-job

# Ver todos os jobs
kubectl get jobs -n brewer

# Deletar job antigo e recriar
kubectl delete job flyway-repair-job -n brewer
kubectl apply -f k8s/base/flyway-repair-job.yaml
```

### PodDisruptionBudget est√° bloqueando drain

```bash
# Ver status do PDB
kubectl get pdb -n brewer
kubectl describe pdb brewer-app-pdb -n brewer

# Se precisar drenar n√≥ mesmo assim (cuidado!)
kubectl drain <node-name> --ignore-daemonsets --delete-emptydir-data --disable-eviction
```

## üîó Recursos √öteis

- [AWS EKS Documentation](https://docs.aws.amazon.com/eks/)
- [Kubernetes Documentation](https://kubernetes.io/docs/)
- [AWS Load Balancer Controller](https://kubernetes-sigs.github.io/aws-load-balancer-controller/)
- [Metrics Server](https://github.com/kubernetes-sigs/metrics-server)

## üìù Checklist de Deploy

### Infraestrutura
- [ ] ECR repository criado
- [ ] EKS cluster configurado e rodando
- [ ] kubectl configurado (conex√£o com cluster)
- [ ] Nginx Ingress Controller instalado
- [ ] Metrics Server instalado (autom√°tico via CI/CD)

### Aplica√ß√£o
- [ ] Imagem Docker buildada e pushed para ECR
- [ ] Namespace `brewer` criado
- [ ] ResourceQuota aplicado
- [ ] Secrets configurados (DATABASE_PASSWORD, AWS credentials, MAIL)
- [ ] ConfigMap ajustado para seu ambiente
- [ ] Database (RDS MySQL) configurado e acess√≠vel
- [ ] S3 bucket configurado para fotos

### Deploy
- [ ] Deployment aplicado (com sed para image tag)
- [ ] Service (ClusterIP) criado
- [ ] HPA configurado e funcionando
- [ ] PodDisruptionBudget aplicado
- [ ] Ingress configurado (brewer.virosistemas.com)
- [ ] Pods rodando e healthy (min 2 replicas)

### Verifica√ß√£o
- [ ] `kubectl get pods -n brewer` mostra pods Running
- [ ] `kubectl get hpa -n brewer` mostra m√©tricas
- [ ] `kubectl top pods -n brewer` mostra CPU/Memory
- [ ] Ingress acess√≠vel via dom√≠nio
- [ ] Health check responde: `curl http://brewer.virosistemas.com/actuator/health`
- [ ] Logs sem erros: `kubectl logs -f deployment/brewer-app -n brewer`
- [ ] ResourceQuota dentro dos limites
- [ ] PDB protegendo pods

### Seguran√ßa
- [ ] Rate limiting configurado no Ingress
- [ ] Actuator metrics isolados (porta 9090)
- [ ] Session cookie com 8h de dura√ß√£o
- [ ] Secrets n√£o commitados no Git

## üéØ Pr√≥ximos Passos

### Conclu√≠do
- [x] CI/CD configurado (GitHub Actions)
- [x] Auto-scaling implementado (HPA)
- [x] Rate limiting e prote√ß√£o DDoS
- [x] Seguran√ßa dos endpoints Actuator
- [x] ResourceQuota e PodDisruptionBudget
- [x] Metrics Server para monitoramento b√°sico

### Recomendado
1. **Configurar SSL/TLS**
   - Obter certificado SSL via Cloudflare ou AWS Certificate Manager
   - Habilitar TLS no Ingress
   - For√ßar HTTPS redirect

2. **Implementar monitoramento avan√ßado**
   - Prometheus para coleta de m√©tricas
   - Grafana para dashboards
   - Loki para agrega√ß√£o de logs

3. **Configurar alertas**
   - AlertManager para alertas do Prometheus
   - CloudWatch Alarms para m√©tricas AWS
   - PagerDuty/Slack integration

4. **Backup autom√°tico**
   - RDS automated backups (j√° incluso se usar RDS)
   - S3 bucket versioning para fotos
   - Velero para backup do cluster

5. **Disaster Recovery**
   - Multi-AZ RDS (alta disponibilidade)
   - Cross-region replication do S3
   - EKS cluster backup strategy

6. **Observabilidade**
   - Distributed tracing (Jaeger ou AWS X-Ray)
   - APM (Application Performance Monitoring)
   - Error tracking (Sentry)

7. **Melhorias de Seguran√ßa**
   - Migrar secrets para AWS Secrets Manager
   - Implementar Network Policies
   - Pod Security Standards
   - Vulnerability scanning (Trivy, Snyk)

8. **Performance**
   - Configurar CDN (CloudFront) para assets est√°ticos
   - Implementar Redis para cache
   - Database query optimization
